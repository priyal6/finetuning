# -*- coding: utf-8 -*-
"""LoRA- low rank adaptation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16HxSMj2RvPeLhW06E_R4w3bBBqFLXHzQ
"""

pip install -q torch transformers datasets accelerate peft evaluate sentencepiece safetensors

import os
from dataclasses import dataclass
from typing import Optional
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)

from datasets import Dataset
from peft import(
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)

@dataclass
class Config:
  model_name: str = "gpt2"
  output_dir: str = "lora-gpt2-output"
  per_device_train_batch_size: int = 4
  num_train_epochs: int = 3
  learning_rate: float = 2e-4
  weight_decay: float = 0.0
  fp16: bool = False
  lora_r: int = 8
  lora_alpha: int = 32
  lora_dropout: float = 0.1

  max_seq_length: int = 256

cfg = Config()

texts = [
    "Hello, my name is Ada and I love cats.",
    "Weather today: sunny with a chance of learning.",
    "Data science is about asking the right questions and checking assumptions.",
    "Fine-tuning language models with LoRA can be fast and cheap if done correctly."
]

dataset = Dataset.from_dict({"text": texts})

tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)

#adding pad tokens
if tokenizer.pad_token_id is None:
  tokenizer.add_special_tokens({"pad_token": "[PAD]"})

model = AutoModelForCausalLM.from_pretrained(cfg.model_name)

#as the embedding matrix acts as a lookup table for the token input ids - as each token id is vector in dimensional space
#both should be of equal length or else it will crash
if tokenizer.pad_token_id is not None and model.get_input_embeddings().weight.shape[0]!= len(tokenizer):
  model.resize_token_embeddings(len(tokenizer))

lora_config = LoraConfig(
    r = cfg.lora_r,
    lora_alpha = cfg.lora_alpha,
    target_modules = ["c_attn", "c_proj"],
    lora_dropout = cfg.lora_dropout,
    bias = "none",
    task_type = "CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

def tokenize_fn(examples):
  return tokenizer(examples['text'], truncation=True, max_length = cfg.max_seq_length, padding = "max_length")

tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

tokenized = tokenized.map(lambda ex: {"labels": ex["input_ids"]}, batched=False)

data_collator = DataCollatorForLanguageModeling(
    tokenizer = tokenizer,
    mlm = False
)

training_args = TrainingArguments(
    output_dir = cfg.output_dir,
    num_train_epochs = cfg.num_train_epochs,
    per_device_train_batch_size = cfg.per_device_train_batch_size,
    learning_rate = cfg.learning_rate,
    weight_decay = cfg.weight_decay,
    fp16 = cfg.fp16,
    logging_steps = 10,
    save_total_limit = 2,
    save_strategy = "epoch",
    push_to_hub = False,
    report_to = "none"
)

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized,
    data_collator = data_collator,
)

trainer.train()

os.makedirs(cfg.output_dir, exist_ok=True)
model.save_pretrained(cfg.output_dir)
tokenizer.save_pretrained(cfg.output_dir)
print(f"Finished. Saved LoRA adapters and tokenizer to {cfg.output_dir}")

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


base = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({"pad_token": "[PAD]"})
base.resize_token_embeddings(len(tokenizer))

model = PeftModel.from_pretrained(base, "lora-gpt2-output")

#inference
input_text = "Data science means"
inputs = tokenizer(input_text, return_tensors = "pt")
with torch.no_grad():
  out = model.generate(**inputs, max_length = 50)
print(tokenizer.decode(out[0],skip_special_tokens=True ))

