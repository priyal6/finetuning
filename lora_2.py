# -*- coding: utf-8 -*-
"""LoRA 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bqzZ66ofO3A42cuDDCsq24wndPfU--Z4
"""

import os
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling

!pip install -q bitsandbytes datasets accelerate loralib
!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git

import torch
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig


quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

import torch.nn as nn
import torch
for param in model.parameters():
  param.requires_grad = False

model.gradient_checkpointing_enable()
model.enable_input_require_grads()

class CastOutputToFloat(nn.Sequential):
  def forward(self, x):
    return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)

trainable_params = 0
all_param = 0
for _, param in model.named_parameters():
  all_param += param.numel()
  if param.requires_grad:
    trainable_params += param.numel()

print(f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}")

for name, module in model.named_modules():
    if 'attn' in name or 'attention' in name:  # Common attention module names
        print(name)
        for sub_name, sub_module in module.named_modules():  # Check sub-modules within attention
            print(f"  - {sub_name}")

from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r = 16,
    lora_alpha = 32,
    target_modules = ["k_proj", "v_proj"],
    lora_dropout = 0.05,
    bias = "none",
    task_type = "CAUSAL_LM"
)

lora_model = get_peft_model(model, config)

trainable_params = 0
all_param = 0
for _, param in lora_model.named_parameters():
  all_param += param.numel()
  if param.requires_grad:
    trainable_params += param.numel()

print(f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}")

import transformers
from datasets import load_dataset
data = load_dataset("Abirate/english_quotes")

def merge_columns(example):
  example['prediction'] = example['quote'] + " ->:" + str(example['tags'])
  return example

data['train'] = data['train'].map(merge_columns)
data['train']['prediction'][0]

data = data.map(lambda samples: tokenizer(samples['prediction']), batched = True)

trainer = transformers.Trainer(
    model = lora_model,
    train_dataset = data['train'],
    args = transformers.TrainingArguments(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 1,
        warmup_steps = 10,
        max_steps = 3,
        learning_rate = 2e-4,
        fp16 = False,
        logging_steps = 1,
        output_dir = 'outputs'

    ),
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)
)
model.config.use_cache = False
trainer.train()

"""Let's perform an inference with the fine-tuned LoRA model."""

eval_prompt = "Tell me about the importance of being earnest. ->: "


device = "cuda" if torch.cuda.is_available() else "cpu"
model_input = tokenizer(eval_prompt, return_tensors="pt").to(device)

lora_model.eval()
with torch.no_grad():
    print(tokenizer.decode(lora_model.generate(**model_input, max_new_tokens=50)[0], skip_special_tokens=True))